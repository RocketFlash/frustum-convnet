{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from waymo_open_dataset import dataset_pb2 as open_dataset\n",
    "import waymo.waymo_utils as wutils\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from kitti import kitti_util as utils\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     8
    ]
   },
   "outputs": [],
   "source": [
    "row_idx = 5\n",
    "pred_file_idx = 10\n",
    "PREDS_PATH = '/root/workdir/waymo_challenge/predictions/'\n",
    "PREDS_FILE = PREDS_PATH + f'pred_{pred_file_idx}.csv'\n",
    "PREDS_PATH_2D = '/dataset/kitti_format/waymo/waymo_challenge_3/validation/preds_3/'\n",
    "PREDS_FILE_2D = PREDS_PATH_2D + 'pred_all.csv'\n",
    "TFRECORDS_PATH = '/dataset/waymo/validation/'\n",
    "\n",
    "colors = [(255,0,0),\n",
    "         (0,0,255),\n",
    "         (0,255,0),\n",
    "         (255,0,255),\n",
    "         (0,255,255)]\n",
    "\n",
    "df = pd.read_csv(PREDS_FILE) \n",
    "predictions_df_2d = pd.read_csv(PREDS_FILE_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     41
    ]
   },
   "outputs": [],
   "source": [
    "class Object3d(object):\n",
    "    ''' 3d object label '''\n",
    "\n",
    "    def __init__(self,obj):\n",
    "        class_type = int(obj[1])\n",
    "        x1,y1 = int(obj[2]), int(obj[3])\n",
    "        x2,y2 = int(obj[4]), int(obj[5])\n",
    "        tx, ty, tz = obj[6], obj[7], obj[8]\n",
    "        h, w, l = obj[9], obj[10], obj[11]\n",
    "        ry = obj[12]\n",
    "        \n",
    "        self.type = class_type  # 'Car', 'Pedestrian', ...\n",
    "        self.truncation = 0  # truncated pixel ratio [0..1]\n",
    "        self.occlusion = 0 # 0=visible, 1=partly occluded, 2=fully occluded, 3=unknown\n",
    "        self.alpha = 0  # object observation angle [-pi..pi]\n",
    "\n",
    "        # extract 2d bounding box in 0-based coordinates\n",
    "        self.xmin = x1  # left\n",
    "        self.ymin = y1  # top\n",
    "        self.xmax = x2  # right\n",
    "        self.ymax = y2  # bottom\n",
    "        self.box2d = np.array([self.xmin, self.ymin, self.xmax, self.ymax])\n",
    "\n",
    "        # extract 3d bounding box information\n",
    "        self.h = h  # box height\n",
    "        self.w = w # box width\n",
    "        self.l = l # box length (in meters)\n",
    "        self.t = (tx, ty, tz)  # location (x,y,z) in camera coord.\n",
    "        self.ry = ry  # yaw angle (around Y-axis in camera coordinates) [-pi..pi]\n",
    "\n",
    "        self.score = 1\n",
    "\n",
    "def get_box_transformation_matrix_my(obj_loc, obj_size, ry):\n",
    "    \"\"\"Create a transformation matrix for a given label box pose.\"\"\"\n",
    "\n",
    "    tx,ty,tz = obj_loc\n",
    "    c = math.cos(ry)\n",
    "    s = math.sin(ry)\n",
    "\n",
    "    sl, sh, sw = obj_size\n",
    "\n",
    "    return np.array([\n",
    "        [       -sl*c,    0,  sw*s,    tx],\n",
    "        [          0,  -sh,    0,      ty],\n",
    "        [    -sl*(-s),    0,  sw*c,    tz],\n",
    "        [          0,    0,    0,     1]])\n",
    "\n",
    "def visualize_2d_predictions(predictions_df_2d, frame_img_name, image, confidence_thresh_2d=0.5):\n",
    "    pred_df_row = predictions_df_2d.loc[predictions_df_2d['image_name'] == frame_img_name]\n",
    "    bboxes = pred_df_row['bboxes'].iloc[0].split(',')\n",
    "    bboxes_float = [[float(bbox_el) for bbox_el in bbox[1:-1].split(' ')] for bbox in bboxes]\n",
    "\n",
    "    for bbox in bboxes_float:\n",
    "        center_x, center_y, w, h, pred_class, score = bbox\n",
    "        if score >= confidence_thresh_2d:\n",
    "            x1, y1, x2, y2 = int(center_x - w/2), int(center_y - h/2), int(center_x + w/2), int(center_y + h/2)\n",
    "            cv2.rectangle(\n",
    "                    image,\n",
    "                    (x1, y1),\n",
    "                    (x2, y2),\n",
    "                    colors[int(pred_class)],\n",
    "                    2,\n",
    "                )\n",
    "\n",
    "    fig=plt.figure(figsize=(32, 16))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "def visualize_2d_frustum(image, objects_in_cam):\n",
    "    for obj in objects_in_cam:\n",
    "        cv2.rectangle(\n",
    "                image,\n",
    "                (int(obj[2]), int(obj[3])),\n",
    "                (int(obj[4]), int(obj[5])),\n",
    "                colors[int(obj[1])],\n",
    "                2,\n",
    "            )\n",
    "\n",
    "    fig=plt.figure(figsize=(32, 16))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "def visualize_3d_frustum(image, calib, objects_in_cam, filter_class=[0,1,2,3]):\n",
    "    objects_in_cam = objects_by_cam_id[cam_idx]\n",
    "\n",
    "    for obj in objects_in_cam:\n",
    "        obj_t = Object3d(obj)\n",
    "        if obj_t.type in filter_class:\n",
    "            box3d_pts_2d, box3d_pts_3d = utils.compute_box_3d(obj_t, calib.P)\n",
    "            if box3d_pts_2d is None or box3d_pts_3d is None:\n",
    "                print('Skip box')\n",
    "                continue\n",
    "            image = utils.draw_projected_box3d(image, box3d_pts_2d, color=colors[obj_t.type])\n",
    "\n",
    "    fig=plt.figure(figsize=(32, 16))\n",
    "    plt.imshow(image)\n",
    "    \n",
    "def show_interactive_lidar(lidar_scan, calib=None, objects=[], with_boxes=False,to_show=True):\n",
    "    \n",
    "    Tr_V2C = np.eye(4)\n",
    "    Tr_V2C[:3,:] = calib.V2C\n",
    "    lidar_scan = np.matmul(Tr_V2C, lidar_scan.T).T\n",
    "    \n",
    "    distances = np.sqrt(np.sum(lidar_scan**2, axis=1))\n",
    "    scatter = go.Scatter3d(x=lidar_scan[:,2], y=-lidar_scan[:,0], z=-lidar_scan[:,1],\n",
    "                                   mode='markers',marker=dict(size=2,                # set color to an array/list of desired values\n",
    "                                                              color=distances,\n",
    "                                                              colorscale='Viridis',   # choose a colorscale\n",
    "                                                              opacity=0.8\n",
    "                                                            ))\n",
    "    if with_boxes:\n",
    "        x_lines = []\n",
    "        y_lines = []\n",
    "        z_lines = []\n",
    "\n",
    "        def f_lines_add_nones():\n",
    "            x_lines.append(None)\n",
    "            y_lines.append(None)\n",
    "            z_lines.append(None)\n",
    "\n",
    "        ixs_box_0 = [0, 1, 2, 3, 0]\n",
    "        ixs_box_1 = [4, 5, 6, 7, 4]\n",
    "        axes_transformation = np.array([[0,-1,0,0],\n",
    "                                        [0,0,-1,0],\n",
    "                                        [1,0,0,0],\n",
    "                                        [0,0,0,1]])\n",
    "        axes_transformation_inv = np.linalg.inv(axes_transformation)\n",
    "        for obj_i in objects:\n",
    "            obj = Object3d(obj_i)\n",
    "#             points = view_points(box.corners(), view=np.eye(3), normalize=False)\n",
    "            Tr_2box = get_box_transformation_matrix_my(obj.t, (obj.l, obj.h, obj.w), obj.ry)\n",
    "#             Tr_2box = np.matmul(Tr_2box,axes_transformation)\n",
    "\n",
    "            x_corners =  np.array([1/2, -1/2, -1/2, 1/2, 1/2, -1/2, -1/2, 1/2])\n",
    "            y_corners =  np.array([1, 1, 0, 0, 1, 1, 0, 0])\n",
    "            z_corners =  np.array([1/2, 1/2, 1/2, 1/2, -1/2, -1/2, -1/2, -1/2])            \n",
    "            \n",
    "            corners = np.vstack((x_corners, y_corners, z_corners, np.ones(8)))\n",
    "            points = np.matmul(Tr_2box, corners)\n",
    "            \n",
    "            x_lines.extend(points[0, ixs_box_0])\n",
    "            y_lines.extend(points[1, ixs_box_0])\n",
    "            z_lines.extend(points[2, ixs_box_0])\n",
    "            f_lines_add_nones()\n",
    "            x_lines.extend(points[0, ixs_box_1])\n",
    "            y_lines.extend(points[1, ixs_box_1])\n",
    "            z_lines.extend(points[2, ixs_box_1])\n",
    "            f_lines_add_nones()\n",
    "            for i in range(4):\n",
    "                x_lines.extend(points[0, [ixs_box_0[i], ixs_box_1[i]]])\n",
    "                y_lines.extend(points[1, [ixs_box_0[i], ixs_box_1[i]]])\n",
    "                z_lines.extend(points[2, [ixs_box_0[i], ixs_box_1[i]]])\n",
    "                f_lines_add_nones()\n",
    "        y_lines = [-1*val if val is not None else val for val in y_lines]\n",
    "        x_lines = [-1*val if val is not None else val for val in x_lines]\n",
    "        lines = go.Scatter3d(x=z_lines, y=x_lines, z=y_lines, mode=\"lines\", name=\"lines\")\n",
    "        fig = go.Figure(data=[scatter, lines])\n",
    "    else:\n",
    "        fig = go.Figure(data=[scatter])\n",
    "    fig.update_layout(scene_aspectmode=\"data\")\n",
    "    if to_show:\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = df.iloc[row_idx]\n",
    "\n",
    "tfrecord_name = frame['tfrecord_name']\n",
    "tfrecord_path = TFRECORDS_PATH + tfrecord_name\n",
    "tfrec_idx = frame['tfrec_idx']\n",
    "frame_idx = frame['frame_idx']\n",
    "predictions = frame['predictions']\n",
    "\n",
    "print(predictions)\n",
    "preds = predictions.split(',')\n",
    "objects = [[float(pred_el) for pred_el in pred[1:-1].split(' ')] for pred in preds]\n",
    "\n",
    "objects_by_cam_id = {1: [],\n",
    "                     2: [],\n",
    "                     3: [],\n",
    "                     4: [],\n",
    "                     5: []}\n",
    "for obj in objects:\n",
    "    objects_by_cam_id[int(obj[0])].append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(tfrecord_path, compression_type='')\n",
    "frame = open_dataset.Frame()\n",
    "\n",
    "for idx, data_i in enumerate(dataset):\n",
    "    if idx == frame_idx:     \n",
    "        frame.ParseFromString(bytearray(data_i.numpy()))\n",
    "\n",
    "images = wutils.get_images(frame)\n",
    "calibs = wutils.get_calibs(frame)\n",
    "pc = wutils.get_lidar(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "CONFIDENCE_THRESH_2D = 0.5\n",
    "cam_idx = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## 2D Predictions from mmdetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "frame_img_name = f'{tfrec_idx}_{frame_idx}_{cam_idx}.jpg'\n",
    "image = images[cam_idx-1].copy()\n",
    "visualize_2d_predictions(predictions_df_2d, frame_img_name, image, confidence_thresh_2d=CONFIDENCE_THRESH_2D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Visualize frustum 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image = images[cam_idx-1].copy()\n",
    "objects_in_cam = objects_by_cam_id[cam_idx]\n",
    "\n",
    "visualize_2d_frustum(image, objects_in_cam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Visualize frustum 3D on image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "image = images[cam_idx-1].copy()\n",
    "calib = calibs[cam_idx-1]\n",
    "objects_in_cam = objects_by_cam_id[cam_idx]\n",
    "\n",
    "visualize_3d_frustum(image, calib, objects_in_cam, filter_class=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Visualize frustum 3D on pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "show_interactive_lidar(pc, calib=calibs[cam_idx-1], objects=objects_by_cam_id[cam_idx], with_boxes=True,to_show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Cut region from pointcloud based on 2d detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     14,
     51,
     57,
     63,
     69,
     76,
     86,
     96
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from waymo.prepare_data_waymo import extract_frustum_data_inference\n",
    "from datasets.dataset_info import WaymoCategory\n",
    "from datasets.data_utils import rotate_pc_along_y, project_image_to_rect, compute_box_3d, extract_pc_in_box3d, roty\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import open3d as o3d\n",
    "\n",
    "def generate_ref(box, P):\n",
    "    s1, s2, s3, s4 = (0.1, 0.2, 0.4, 0.8)\n",
    "\n",
    "    z1 = np.arange(0, 70, s1) + s1 / 2.\n",
    "    z2 = np.arange(0, 70, s2) + s2 / 2.\n",
    "    z3 = np.arange(0, 70, s3) + s3 / 2.\n",
    "    z4 = np.arange(0, 70, s4) + s4 / 2.\n",
    "\n",
    "    cx, cy = (box[0] + box[2]) / 2., (box[1] + box[3]) / 2.,\n",
    "\n",
    "    xyz1 = np.zeros((len(z1), 3))\n",
    "    xyz1[:, 0] = cx\n",
    "    xyz1[:, 1] = cy\n",
    "    xyz1[:, 2] = z1\n",
    "    xyz1_rect = project_image_to_rect(xyz1, P)\n",
    "\n",
    "    xyz2 = np.zeros((len(z2), 3))\n",
    "    xyz2[:, 0] = cx\n",
    "    xyz2[:, 1] = cy\n",
    "    xyz2[:, 2] = z2\n",
    "    xyz2_rect = project_image_to_rect(xyz2, P)\n",
    "\n",
    "    xyz3 = np.zeros((len(z3), 3))\n",
    "    xyz3[:, 0] = cx\n",
    "    xyz3[:, 1] = cy\n",
    "    xyz3[:, 2] = z3\n",
    "    xyz3_rect = project_image_to_rect(xyz3, P)\n",
    "\n",
    "    xyz4 = np.zeros((len(z4), 3))\n",
    "    xyz4[:, 0] = cx\n",
    "    xyz4[:, 1] = cy\n",
    "    xyz4[:, 2] = z4\n",
    "    xyz4_rect = project_image_to_rect(xyz4, P)\n",
    "\n",
    "    return xyz1_rect, xyz2_rect, xyz3_rect, xyz4_rect\n",
    "\n",
    "\n",
    "def get_center_view_rot_angle(frustum_angle):\n",
    "    ''' Get the frustum rotation angle, it isshifted by pi/2 so that it\n",
    "    can be directly used to adjust GT heading angle '''\n",
    "    return np.pi / 2.0 + frustum_angle\n",
    "\n",
    "\n",
    "def get_box3d_center(box3d):\n",
    "    ''' Get the center (XYZ) of 3D bounding box. '''\n",
    "    box3d_center = (box3d[0, :] + box3d[6, :]) / 2.0\n",
    "    return box3d_center\n",
    "\n",
    "\n",
    "def get_center_view_box3d_center(box3d, frustum_angle):\n",
    "    ''' Frustum rotation of 3D bounding box center. '''\n",
    "    box3d_center = (box3d[0, :] + box3d[6, :]) / 2.0\n",
    "    return rotate_pc_along_y(np.expand_dims(box3d_center, 0), get_center_view_rot_angle(frustum_angle)).squeeze()\n",
    "\n",
    "\n",
    "def get_center_view_box3d(box3d, frustum_angle):\n",
    "    ''' Frustum rotation of 3D bounding box corners. '''\n",
    "    box3d = box3d\n",
    "    box3d_center_view = np.copy(box3d)\n",
    "    return rotate_pc_along_y(box3d_center_view, get_center_view_rot_angle(frustum_angle))\n",
    "\n",
    "\n",
    "def get_center_view_point_set(input_i, frustum_angle):\n",
    "    ''' Frustum rotation of point clouds.\n",
    "    NxC points with first 3 channels as XYZ\n",
    "    z is facing forward, x is left ward, y is downward\n",
    "    '''\n",
    "    # Use np.copy to avoid corrupting original data\n",
    "    point_set = np.copy(input_i)\n",
    "    return rotate_pc_along_y(point_set, get_center_view_rot_angle(frustum_angle))\n",
    "\n",
    "\n",
    "def get_center_view(point_set, frustum_angle):\n",
    "    ''' Frustum rotation of point clouds.\n",
    "    NxC points with first 3 channels as XYZ\n",
    "    z is facing forward, x is left ward, y is downward\n",
    "    '''\n",
    "    # Use np.copy to avoid corrupting original data\n",
    "    point_set = np.copy(point_set)\n",
    "    return rotate_pc_along_y(point_set, get_center_view_rot_angle(frustum_angle))\n",
    "\n",
    "\n",
    "def get_frustum_input(data,  one_hot=True, npoints=1024, min_n_points = 5):\n",
    "    box2d_i = data['box2d_i']\n",
    "    input_i = data['input_i']\n",
    "    type_i = data['type_i']\n",
    "    frustum_angle_i = data['frustum_angle_i']\n",
    "    calib_i = data['calib_i']\n",
    "\n",
    "    rotate_to_center = True\n",
    "    with_extra_feat = False\n",
    "\n",
    "    ''' Get index-th element from the picked file dataset. '''\n",
    "    # ------------------------------ INPUTS ----------------------------\n",
    "    rot_angle = get_center_view_rot_angle(frustum_angle_i )\n",
    "\n",
    "    cls_type = type_i\n",
    "    assert cls_type in WaymoCategory.CLASSES, cls_type\n",
    "    size_class = WaymoCategory.CLASSES.index(cls_type)\n",
    "\n",
    "    # Compute one hot vector\n",
    "    if one_hot:\n",
    "        one_hot_vec = np.zeros((3))\n",
    "        one_hot_vec[size_class] = 1\n",
    "\n",
    "    # Get point cloud\n",
    "    if rotate_to_center:\n",
    "        point_set = get_center_view_point_set(input_i, frustum_angle_i)\n",
    "    else:\n",
    "        point_set = input_i \n",
    "\n",
    "    if not with_extra_feat:\n",
    "        point_set = point_set[:, :3]\n",
    "\n",
    "    if point_set.shape[0]<= min_n_points:\n",
    "        # print(f'No enougth number of points {point_set.shape[0]}')\n",
    "        return None\n",
    "    # Resample\n",
    "    if npoints > 0:\n",
    "        choice = np.random.choice(point_set.shape[0], npoints, point_set.shape[0] < npoints)\n",
    "    else:\n",
    "        choice = np.random.permutation(len(point_set.shape[0]))\n",
    "\n",
    "    point_set = point_set[choice, :]\n",
    "\n",
    "    box = box2d_i\n",
    "    P = calib_i.P\n",
    "\n",
    "    ref1, ref2, ref3, ref4 = generate_ref(box, P)\n",
    "\n",
    "    if rotate_to_center:\n",
    "        ref1 = get_center_view(ref1, frustum_angle_i)\n",
    "        ref2 = get_center_view(ref2, frustum_angle_i)\n",
    "        ref3 = get_center_view(ref3, frustum_angle_i)\n",
    "        ref4 = get_center_view(ref4, frustum_angle_i)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(ref1)\n",
    "    o3d.io.write_point_cloud(\"xyz1.pcd\", pcd)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(ref2)\n",
    "    o3d.io.write_point_cloud(\"xyz2.pcd\", pcd)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(ref3)\n",
    "    o3d.io.write_point_cloud(\"xyz3.pcd\", pcd)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(ref4)\n",
    "    o3d.io.write_point_cloud(\"xyz4.pcd\", pcd)\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(point_set)\n",
    "    o3d.io.write_point_cloud(\"pedestrian.pcd\", pcd)\n",
    "\n",
    "    data_inputs = {\n",
    "        'point_cloud': torch.FloatTensor(point_set).transpose(1, 0),\n",
    "        'rot_angle': torch.FloatTensor([rot_angle]),\n",
    "        'center_ref1': torch.FloatTensor(ref1).transpose(1, 0),\n",
    "        'center_ref2': torch.FloatTensor(ref2).transpose(1, 0),\n",
    "        'center_ref3': torch.FloatTensor(ref3).transpose(1, 0),\n",
    "        'center_ref4': torch.FloatTensor(ref4).transpose(1, 0),\n",
    "    }\n",
    "\n",
    "    if not rotate_to_center:\n",
    "        data_inputs.update({'rot_angle': torch.zeros(1)})\n",
    "\n",
    "    if one_hot:\n",
    "        data_inputs.update({'one_hot': torch.FloatTensor(one_hot_vec)})\n",
    "    \n",
    "    return data_inputs\n",
    "\n",
    "frame_img_name = f'{tfrec_idx}_{frame_idx}_{cam_idx}.jpg'\n",
    "pred_df_row = predictions_df_2d.loc[predictions_df_2d['image_name'] == frame_img_name]\n",
    "bboxes = pred_df_row['bboxes'].iloc[0].split(',')\n",
    "bboxes_float = [[float(bbox_el) for bbox_el in bbox[1:-1].split(' ')] for bbox in bboxes]\n",
    "image = images[cam_idx-1].copy()\n",
    "\n",
    "for bbox in bboxes_float:\n",
    "        center_x, center_y, w, h, pred_class, score = bbox\n",
    "        if score >= CONFIDENCE_THRESH_2D and pred_class==2:\n",
    "            x1, y1, x2, y2 = int(center_x - w/2), int(center_y - h/2), int(center_x + w/2), int(center_y + h/2)\n",
    "            data = extract_frustum_data_inference([x1, y1, x2, y2, 'PEDESTRIAN', 1], pc, images[cam_idx-1], calibs[cam_idx-1], type_whitelist=['PEDESTRIAN'])\n",
    "            data_dict = get_frustum_input(data)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
